{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from math import exp\n",
    "from scipy.optimize import fmin_tnc\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction qui retourne 0 ou 1 en fonction la valeur de la prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/Benlau93/Machine-Learning-by-Andrew-Ng-in-Python/blob/master/LogisticRegression/ML_LogisticRegression.ipynb\n",
    "def predictions(theta , X):\n",
    "    prev = []\n",
    "    for lig in X:\n",
    "        res = sigmoid(np.transpose(theta) @ np.transpose(lig))\n",
    "        if res >= 0.5 :\n",
    "            prev.append(1)\n",
    "        else:\n",
    "            prev.append(0)\n",
    "    return prev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fonction d activation sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "        g = 1/(1 + np.exp(-1 * z))\n",
    "        return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fonction de cout et calcul du gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y): \n",
    "    m=len(y)\n",
    "    predictions = sigmoid(np.dot(X,theta))\n",
    "    error = (-y * np.log(predictions)) - ((1-y)*np.log(1-predictions))\n",
    "    cost = 1/m * sum(error)\n",
    "    grad = 1/m * np.dot(X.transpose(),(predictions - y))\n",
    "    return cost[0] , grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fonction de cout uniquememt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_cost(theta, X, y):\n",
    "    m=len(y)\n",
    "    predictions = sigmoid(np.dot(X,theta))\n",
    "    error = (-y * np.log(predictions)) - ((1-y)*np.log(1-predictions))\n",
    "    cost = 1/m * sum(error)\n",
    "    return cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fonction de gradient uniquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_gradient(theta, X, y):\n",
    "     predictions = sigmoid(np.dot(X,theta))\n",
    "     grad = 1/m * np.dot(X.transpose(),(predictions - y))    \n",
    "     return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fonction de normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalization(X):\n",
    "    \"\"\"\n",
    "    Take in numpy array of X values and return normalize X values,\n",
    "    the mean and standard deviation of each feature\n",
    "    \"\"\"\n",
    "    mean=np.mean(X,axis=0)\n",
    "    std=np.std(X,axis=0)\n",
    "    X_norm = (X - mean)/std\n",
    "    return X_norm , mean , std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch de descente en gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,num_iters):   \n",
    "    m=len(y)\n",
    "    J_history =[]\n",
    "    for i in range(num_iters):\n",
    "        cost, grad = costFunction(theta,X,y)\n",
    "        theta = theta - (alpha * grad)\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    return theta , J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chargememt des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ex2data1.txt', header=None,sep =',', names= ['x1','x2','y'])\n",
    "recus   = data.loc[data['y']==  1]\n",
    "recales = data.loc[data['y']==  0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests et initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigmoid(4))\n",
    "t = np.array([1, 2 ,3])\n",
    "print(sigmoid(t))\n",
    "d = pd.DataFrame([1, 2 ,3])\n",
    "print(sigmoid(d))\n",
    "# preparaton des donnees\n",
    "X = data[['x1','x2']]\n",
    "y  = data['y'].to_numpy()\n",
    "#X.insert(0,'x0',1)\n",
    "#Xtranspo = np.vstack((np.ones(M), X)).T\n",
    "X = X.to_numpy()\n",
    "m  = X.shape[0]\n",
    "X, X_mean, X_std = featureNormalization(X)\n",
    "X = np.c_[ np.ones(m), X]\n",
    "n = X.shape[1]\n",
    "theta_0 = np.zeros((n,1))\n",
    "#  pandes style  X= np.append(np.ones((m,1)),X,axis=1)\n",
    "y=y.reshape(m,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calcul par batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_theta = theta_0\n",
    "cost, grad= costFunction(initial_theta,X,y)\n",
    "print(\"Cost of initial theta is\",cost)\n",
    "print(\"Gradient at initial theta (zeros):\",grad)\n",
    "theta , J_history = gradientDescent(X,y,initial_theta,1,400)\n",
    "print(\"Theta optimized by gradient descent:\",theta)\n",
    "print(\"The cost of the optimized theta:\",J_history[-1])\n",
    "x_test = np.array([45,85])\n",
    "x_test = (x_test - X_mean)/X_std\n",
    "x_test = np.append(np.ones(1),x_test)\n",
    "prob = sigmoid(x_test.dot(theta))\n",
    "print(\"For a student with scores 45 and 85, we predict an admission probability of\",prob[0])\n",
    "print(\"methode unitaire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calcul par fonction de recherche de minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cout =  f_cost(initial_theta,X,y)\n",
    "gradient = f_gradient(initial_theta,X,y)\n",
    "print(\"cost\",cout)\n",
    "print(\"gradient\", gradient)\n",
    "essai = fmin_tnc(func = f_cost, x0 = initial_theta.flatten(), fprime = f_gradient , args = (X , y.flatten() )          )\n",
    "print('theta fmin', essai[0])    \n",
    "prob = sigmoid(x_test.dot(essai[0]))\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = predictions(theta, X)\n",
    "cible = y.flatten()\n",
    "confusion = confusion_matrix(prev,cible,[0,1])\n",
    "print('matrice de confusion',confusion)\n",
    "\n",
    "total  = np.sum(confusion)\n",
    "\n",
    "prec = (confusion[0][0] + confusion[1][1]) / total\n",
    "effic = confusion[1][1]/np.sum(confusion, axis = 0)[1]  \n",
    "print('precision',prec)\n",
    "print('pertinence', effic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
